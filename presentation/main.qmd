---
title: An introduction to Gaussian processes
subtitle: ""
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    css: styles.css
    footer: <Matthew Jones, Dynamics Research Group>

---

## General points

- Have some slides to follow, but the more discussion we can have the better 

- Have tried to go as light on the maths as possible - use these slides as stepping stone

## General points

- Have some slides to follow, but the more discussion we can have the better 

- Have tried to go as light on the maths as possible - use these slides as stepping stone

- Quite a lot of accompanying code beneath the slides, will make it public on github - caution there might be some bugs! 

![](figs/bug.webp){fig-align="center"}


## Why do we care?

- Interested in monitoring an aircraft wing, but only have access to data where we have sensors

- Sensors = cost 

::: {layout-ncol=2}

![](figs/aircraft%20wing/hawk.jpg)

![](figs/aircraft%20wing/hawk-sens.jpg)

:::


## Virtual sensing

- Interested in monitoring an aircraft wing, but only have access to data where we have sensors

- Sensors = cost 

- Use a Gaussian process to act as a "virtual sensor"

![](figs/aircraft%20wing/wing.png)


## Virtual sensing

- Interested in monitoring an aircraft wing, but can only get data where we have sensors

- Sensors = cost 

- Use a Gaussian process to act as a "virtual sensor"

![](figs/aircraft%20wing/wing-vs.png)

## Virtual sensing of different phenomena

- We can extend virtual sensing to using one type of measurement to predict another

- e.g predicting a wind turbine's power output from measured wind speed 


## Virtual sensing of different phenomena

::: {layout-ncol=2 layout-valign="center"}

![](figs/wind-turbine-small.jpg){width=450}

![](figs/powerCurve.png){width=450}

:::


## Many other reasons

- Surrogate modelling

- Dealing with complex behaviours that we don't have physical models for

- 'Digital twins'

## Commonality across tasks

- All revolve around learning a map from $X \rightarrow \mathbf{y}$

- In machine learning & statistics, we call this a regression problem


## Commonality across tasks

- We can use Gaussian process (regression) to learn this map!

- Boils down to fitting curves (functions) through data

- **First Gaussian process definition**; a tool for fitting curves through data

## Fitting curves

```{python}

from scipy.stats import norm
import matplotlib.pyplot as plt
import numpy as np 
from numpy.random import multivariate_normal
import random

import sys 
sys.path.append('../')

import kernels
import models

np.random.seed(20)
x = np.linspace(0,1,100)[:,None]

hyps = np.array([1,.1,1e-6])
kernel = kernels.SquaredExponential(x,x,hyps)
k, _ , _ = kernel.compute_kernel()

mu = np.zeros((100,))
nsamples = 1
y = multivariate_normal(mu, k,nsamples).T

npts = 10
random.seed(1)
randidx = random.sample(range(0,100),npts)

xsample = x[randidx]
ysample = y[randidx]

plt.figure()
plt.plot(xsample,ysample,'kx')

```

## Fitting a GP

```{python}

D = (xsample,ysample)
mu_pred, cov_pred = models.predict(hyps,D,x,kernels.SquaredExponential)
var_pred = np.diag(cov_pred)

plt.figure()
plt.plot(xsample,ysample,'kx',label ='train pts')
plt.plot(x,mu_pred, label ='GP predictions')
plt.fill_between(x[:,0], mu_pred[:,0] - 3*np.sqrt(var_pred), mu_pred[:,0] + 3*np.sqrt(var_pred),color = 'darkgreen', alpha = 0.3, label ='confidence bounds')
plt.legend();
```

## Fitting a GP

```{python}

nsamples = 10
gp_samples = multivariate_normal(mu, cov_pred, nsamples).T
gp_samples += mu_pred

plt.figure()
plt.plot(x,gp_samples)
plt.plot(xsample,ysample,'kx',label ='train pts')
plt.title('Posterior GP samples')
plt.legend();

```


## So how do GPs do it?

![](figs/hzy-ipa.webp){fig-align="center" width=100}


## So how do GPs do it?

::: {layout-ncol=2}

![](figs/hzy-ipa.webp){width=250}

![](figs/beer-recipe.jpg){width=350}


:::

## So how do GPs do it?

::: {layout-ncol=2}

![](figs/hzy-ipa.webp){width=250}

![](figs/hzy-ipa2.webp){width=450}


:::

## So how do GPs do it?

::: {layout-ncol=2}

![](figs/hzy-ipa.webp){width=250}

![](figs/guiness.jpg){width=300}


:::

## The intersection between beer tasting and GPs

- GPs work in the same way! Use similarity between data points to tell us about others

## The intersection between beer tasting and GPs

- GPs work in the same way! Use similarity between data points to tell us about others

- Basis of this notion is treating each data point as a Gaussian distribution

- This mean we have some mean value, as well as some uncertainty on observed data 

## A second definition

Gaussian processes are a flexible tool for performing nonliner regression in a non-parametric, Bayesian manner


## The Gaussian distribution 

```{python}
#initialize a normal distribution with frozen in mean=-1, std. dev.= 1
gaussian1 = norm(loc = 0., scale = 1.0)

x = np.arange(-10, 10, .1)

#plot the pdfs of these normal distributions 
plt.figure()
plt.plot(x, gaussian1.pdf(x));
plt.ylabel('p(x)');
plt.xlabel('x');

```

$p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp^{-\frac{(x-\mu)^2}{2\sigma^2}}$


## The Gaussian distribution

```{python}
gaussian2 = norm(loc = 0., scale = 1)
gaussian3 = norm(loc = 2, scale = 2.5)

plt.figure();
plt.plot(x, gaussian2.pdf(x), x, gaussian3.pdf(x));
plt.ylabel('p(x)');
plt.xlabel('x');

```

$p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp^{-\frac{(x-\mu)^2}{2\sigma^2}}$


## Measuring similarity between the data

```{python}
mu = np.array([0,0])
cov_ind = np.array([[1,0], [0,1]])
cov_cor = np.array([[1,0.85],[0.85,1]])

samples_ind = multivariate_normal(mu,cov_ind, size = 1000)
samples_cor = multivariate_normal(mu,cov_cor, size = 1000)

plt.figure();
fig, axs = plt.subplots(1,2, figsize=(10,6));
axs[0].plot(samples_ind[:,0], samples_ind[:,1],'x', color = 'indigo');
axs[1].plot(samples_cor[:,0], samples_cor[:,1],'x', color = 'indigo');
plt.setp(axs, xlim=(-4,4), ylim=(-4,4));

fig.add_subplot(111, frameon=False);
# hide tick and tick label of the big axes
plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False);
plt.xlabel("x1");
plt.ylabel("x2");
```

## Beyond 2 data points

d = 2

```{python}

np.random.seed(20)
x = np.linspace(0,1,100)[:,None]

hyps = np.array([1,.25])
kernel = kernels.SquaredExponential(x,x,hyps)
k, _ , _ = kernel.compute_kernel()

mu = np.zeros((100,))
nsamples = 5
y = multivariate_normal(mu, k,nsamples).T

d = 2
ysample = y[0::100//d,:]
index = np.linspace(1,d,d)

plt.figure()
plt.plot(index,ysample)

```

## Beyond 2 data points

d = 4

```{python}
d = 4
ysample = y[0::100//d,:]
index = np.linspace(1,d,d)

plt.figure()
plt.plot(index,ysample)

```


## Beyond 2 data points

d = 5

```{python}

d = 5
ysample = y[0::100//d,:]
index = np.linspace(1,d,d)

plt.figure()
plt.plot(index,ysample)

```


## Beyond 2 data points

d = 10

```{python}

d = 10
ysample = y[0::100//d,:]
index = np.linspace(1,d,d)

plt.figure()
plt.plot(index,ysample)

```

## Beyond 2 data points

d = 20

```{python}

d = 20
ysample = y[0::100//d,:]
index = np.linspace(1,d,d)

plt.figure()
plt.plot(index,ysample)

```

## Beyond 2 data points

d = 50

```{python}

d = 50
ysample = y[0::100//d,:]
index = np.linspace(1,d,d)

plt.figure()
plt.plot(index,ysample)
```

## A third & final definition

A Gaussian processes is a collection of random variables such that every subset of those random variables has a multivariate normal distribution

```{python}

d = 100
ysample = y[0::100//d,:]
index = np.linspace(1,d,d)

plt.figure()
plt.plot(index,ysample)
```

## The core ingredients of a GP

- To construct a Gaussian process model, we need to define both 
a mean and a covariance function 

- $f(\mathbf{x}) \sim \mathcal{GP}( m(\mathbf{x}), k (\mathbf{x},\mathbf{x}'))$

## The core ingredients of a GP

- Mean can be viewed as an offset of predictions

- Covariance function controls how "closeness" of data points relates to their function value 

## The covariance function

- Will generally assume a zero mean function

- Many different types of kernel functions!

- Loosely, they govern the "shape" of functions that the GP will model

## Some examples 

```{python}

hyps = [1,0.5]

kernelSE = kernels.SquaredExponential(x,x,hyps)
kernelMA12 = kernels.Matern12(x,x,hyps)

kSE, _ , _ = kernelSE.compute_kernel()
kMA12, _ , _ = kernelMA12.compute_kernel()

mu = np.zeros((100,))
nsamples = 10

ySE = multivariate_normal(mu, kSE,nsamples).T
yMA12 = multivariate_normal(mu, kMA12,nsamples).T

plt.figure();
fig, axs = plt.subplots(1,2, figsize=(10,6));
axs[0].plot(x,ySE)
axs[0].set_title('Samples from SE kernel');
axs[1].plot(x,yMA12)
axs[1].set_title('Samples from Matern 1/2 kernel');

```


## Some examples 

```{python}

hyps = [1,0.5]

kernelMA32 = kernels.Matern32(x,x,hyps)
kernelMA12 = kernels.Matern12(x,x,hyps)

kMA32, _ , _ = kernelMA32.compute_kernel()
kMA12, _ , _ = kernelMA12.compute_kernel()

mu = np.zeros((100,))
nsamples = 10

yMA12 = multivariate_normal(mu, kMA12,nsamples).T
yMA32 = multivariate_normal(mu, kMA32,nsamples).T

plt.figure();
fig, axs = plt.subplots(1,2, figsize=(10,6));
axs[0].plot(x,yMA12)
axs[0].set_title('Samples from Matern 1/2 kernel');
axs[1].plot(x,yMA32)
axs[1].set_title('Samples from Matern 3/2 kernel');

```

## If we have more insight

```{python}

hyps = [1,0.5,0.5]

kernelPeriodic = kernels.Periodic(x,x,hyps)

kPeriodic, _ , _ = kernelPeriodic.compute_kernel()

mu = np.zeros((100,))
nsamples = 10

yPeriodic = multivariate_normal(mu, kPeriodic,nsamples).T

plt.figure()
plt.plot(x,yPeriodic)
plt.title('Samples from periodic kernel');

```

## Breaking down the kernel

- $d = || \mathbf{x} - \mathbf{x}' ||$, where $||\mathbf{p},\mathbf{q}|| = \sqrt{\sum_i{(q_i - p_i)}^2}$

```{python}

# Define the two 2D points
x1, y1 = 2, 3
x2, y2 = 6, 7
eqx, eqy = 4,3.5

middle_x = (x1 + x2) / 2
middle_y = (y1 + y2) / 2
distance = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5

# Plot the two points and the line connecting them
plt.figure()
label_offset = 0.25
plt.plot([x1, x2], [y1, y2], 'go-',markersize = 8)
plt.text(x1 + 0.5*label_offset, y1 + 1.5*label_offset, f'Point 1 \n({x1},{y1})', ha='center')
plt.text(x2 - 1.8*label_offset, y2 - 1*label_offset, f'Point 2 \n({x2},{y2})', ha='center')

plt.text(middle_x, middle_y + 1.5*label_offset, f'd', ha='center', va='top')
plt.text(eqx, eqy, 'd = $\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$', ha = 'left')

# Add labels and title
plt.xlabel('x');
plt.ylabel('y');

```

## Breaking down the kernel

- $d = || \mathbf{x} - \mathbf{x}' ||$, where $||\mathbf{p},\mathbf{q}|| = \sqrt{\sum_i{(q_i - p_i)}^2}$

- $K_{SE}(\mathbf{x}, \mathbf{x}') = \sigma _f^2 \exp( - \frac{d^2}{2 l^2})$

- $K_{MA12}(\mathbf{x}, \mathbf{x}') = \sigma _f^2 \exp( -\frac{d}{l})$

- $K_{MA32}(\mathbf{x}, \mathbf{x}') = \sigma _f^2 ( 1+ \frac{\sqrt{3}d}{l})\exp( - \frac{\sqrt{3}d}{l})$

## Hyperparameters 

- For a given kernel, there will be some hyperparameters that needed to be selected 

- For both the Squared Expoential and Matern family of kernels, these are the lengthscale ($l$) and signal variance $(\sigma _f$)

## Hyperparameters 

```{python}

hyps_long = [1,1] # long lengthscale
hyps_short = [1,0.1] # short lenthscale

kernelSE_long = kernels.SquaredExponential(x,x,hyps_long)
kernelSE_short = kernels.SquaredExponential(x,x,hyps_short)

kSElong, _ , _ = kernelSE_long.compute_kernel()
kSEshort, _ , _ = kernelSE_short.compute_kernel()

mu = np.zeros((100,))
nsamples = 10

ySElong = multivariate_normal(mu, kSElong,nsamples).T
ySEshort = multivariate_normal(mu, kSEshort,nsamples).T

plt.figure()
fig, ax = plt.subplots(1,2)
ax[0].plot(x,ySElong)
ax[0].set_title('lengthscale = 1');
ax[1].plot(x,ySEshort)
ax[1].set_title('lengthscale = 0.1');


```

## How do we set hyperparameters?

- Generally won't know what these values should be

- Bayesian framework in which the GP resides give us access to the marginal likelihood 

- Essentially tells us how well the data fits our model 

## Minimising the marginal likelihood 

- It makes sense for us to select a set of hyperparameters that minimise the *negative log marginal likelihood*

- To compute requires us to evaluate $\log p(\mathbf{y} | X, \boldsymbol{\theta}) = -\frac{1}{2}\mathbf{y}^T\mathbf{K}_{xx}^{-1}\mathbf{y} - \frac{1}{2}\log |\mathbf{K}_{xx}| - \frac{n}{2}\log(2\pi))$

- Balances model fit with complexity (Occam's razor)

- Note that the equation is the +ve log marginal likelihood

## Minimising the marginal likelihood 

```{python}

#| echo: true

def loss(hyps,D,kernel):

    jitter = 1e-6

    xtrain = D[0]
    ytrain = D[1]

    N = len(ytrain)

    hyps = np.exp(hyps)

    sn2 = hyps[-1]

    kxx, _ , _ = kernel(xtrain,xtrain,hyps).compute_kernel()
    kxx += (sn2 * np.eye(N))

    L = np.linalg.cholesky(kxx + jitter*np.eye(N))
    alpha = np.linalg.solve(L.T,np.linalg.solve(L,ytrain)) 

    nlml = (0.5 * ytrain.T @ alpha) + np.sum(np.log(np.diag(L))) + (0.5 * N * np.log(2*np.pi))

    return nlml

```

## Minimising the marginal likelihood 

- We then need to use this loss function inside of a minimisation scheme

- In this case, we will use a gradient descent based algorithm from scipy

```{python}

#| echo: true

def train(hyps,D,kernel):

    opt = minimize(models.loss, hyps, args =(D, kernel), method = 'L-BFGS-B')
    hyps_opt = np.exp(opt.x)
    nlml = opt.fun

    return hyps_opt, nlml

```

- Hyperparameters that minimise the NLML are then spat out as the "optimal" hyperparameters

## A word of warning 

- Optimising GP hyperparameters can be very frustrating..

- Can get stuck in local optima

- Can get around this through a grid search/random initialisation

- Evolutionary optimisation techniques can help here...

## Prediction time

- All that is left is to make some predictions

```{python}

#| echo: true

def predict(hyps,D,xtest,kernel):

    xtrain = D[0]
    ytrain = D[1]
    N = len(ytrain)

    jitter = 1e-6
    sn2 = hyps[-1]

    kxx, kxt, ktt = kernel(xtrain,xtest,hyps).compute_kernel()
    kxx += sn2*np.eye(N)

    L = np.linalg.cholesky(kxx + jitter*np.eye(N))
    alpha = np.linalg.solve(L.T,np.linalg.solve(L,ytrain))

    v = np.linalg.solve(L,kxt)

    mu_pred = kxt.T @ alpha
    cov_pred = ktt - v.T @ v

    return mu_pred, cov_pred
```


## Bringing it all together 

1. Acquire training data $D =(X,\mathbf{y})$

```{python}

#| echo: true

np.random.seed(20)

x = np.linspace(0,2,100)[:,None]
y = np.sin(2*x)**2 * 0.5 * np.cos(2*x)
y += 1e-3 * np.random.normal(0,1, size = (len(y),1))

random.seed(20)
npts = 15
randidx = random.sample(range(0,100),npts)

xtrain = x[randidx]
ytrain = (y[randidx] - np.mean(y[randidx]))/np.std(y[randidx])

D = (xtrain,ytrain)

N = len(xtrain) # num training points 

xtest = x
ytest = (y - np.mean(y[randidx]))/np.std(y[randidx])

```

## Bringing it all together

```{python}

plt.figure()
plt.plot(xtest,ytest)
plt.plot(xtrain,ytrain,'x', label = 'training pts')
plt.legend();

```

## Bringing it all together

2. Select mean and kernel (covariance function)  
    - if have prior knowledge, can put it in here!

```{python}

kernel = kernels.SquaredExponential

```

## Bringing it all together


3. Learn hyperparameters by training the model via minimisation of the *negative marginal log likelihood*

```{python}

#| echo: true

hyps0 = np.array([0.1,0.1,0])
hyps_opt, nlml = models.train(hyps0,D,kernel)
print(hyps_opt)
print(nlml);

```

## Bringing it all together

4. Make predictions by conditioning the model on the training data (shrinking possible functions where we have data)

```{python}

#| echo: true

mu_pred, cov_pred = models.predict(hyps_opt,D,xtest,kernel)
var_pred = np.diag(cov_pred)

```

## Bringing it all together

5. Take over the world! (..not quite)

```{python}

plt.figure()
plt.plot(xtest,mu_pred,label = 'pred')
plt.fill_between(xtest[:,0], (mu_pred + 3*np.sqrt(var_pred))[:,0], (mu_pred - 3*np.sqrt(var_pred))[:,0] ,color ='darkgreen', alpha = 0.3,label = 'confidence')
plt.plot(xtrain,ytrain,'kx',label = 'train')
plt.plot(xtest,ytest,':',label = 'true')
plt.legend();


```

## Drawbacks

.. and there are several despite what you might think I believe after the last hour..

- Training & memory requirements - scale very badly as we increase number of training points
- Selecting a covariance function
- Hyperparameter learning can be challenging; local optima, for more complex kernel structures & if we try and recover full distributions over the parameters 
- We've only considered univariate (1D output) GPs here 

## Concluding remarks

## Bayes rule for GPs

- Prior: $p(f) \sim \mathcal{N}(0,k(x,x'))$
- Likelihood $p(y|f,X) \sim \mathcal{N}(f,\sigma_n^2\mathbb{I})$ - data
- Marginal likelihood $p(y|X)$ - hyperparameter optimisation
- Posterior = Bayes rule = $p(f|y,X) = \frac{\mathcal{N}(0,k(x,x'))\mathcal{N}(f,\sigma_n^2\mathbb{I})}{p(y|X)}$


