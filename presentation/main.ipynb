{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: An introduction to Gaussian processes\n",
        "subtitle: \"\"\n",
        "format:\n",
        "  revealjs: \n",
        "    slide-number: true\n",
        "    chalkboard: \n",
        "      buttons: false\n",
        "    preview-links: auto\n",
        "    css: styles.css\n",
        "    footer: <Matthew Jones, Dynamics Research Group>\n",
        "\n",
        "---"
      ],
      "id": "123afc57"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## General points\n",
        "\n",
        "- Have some slides to follow, but the more discussion we can have the better \n",
        "\n",
        "- Have tried to go as light on the maths as possible \n",
        "\n",
        "\n",
        "## General points\n",
        "\n",
        "- Have some slides to follow, but the more discussion we can have the better \n",
        "\n",
        "- Have tried to go as light on the maths as possible \n",
        "\n",
        "- Quite a lot of accompanying code beneath the slides, will make it public on github - caution there might be some bugs! \n",
        "\n",
        "![](figs/bug.webp){fig-align=\"center\"}\n",
        "\n",
        "\n",
        "## Why do we care?\n",
        "\n",
        "- Interested in monitoring an aircraft wing, but only have access to data where we have sensors\n",
        "\n",
        "- Sensors = cost \n",
        "\n",
        "::: {layout-ncol=2}\n",
        "\n",
        "![](figs/aircraft%20wing/hawk.jpg)\n",
        "\n",
        "![](figs/aircraft%20wing/hawk-sens.jpg)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Virtual sensing\n",
        "\n",
        "- Interested in monitoring an aircraft wing, but only have access to data where we have sensors\n",
        "\n",
        "- Sensors = cost \n",
        "\n",
        "- Use a Gaussian process to act as a \"virtual sensor\"\n",
        "\n",
        "![](figs/aircraft%20wing/wing.png)\n",
        "\n",
        "\n",
        "## Virtual sensing\n",
        "\n",
        "- Interested in monitoring an aircraft wing, but can only get data where we have sensors\n",
        "\n",
        "- Sensors = cost \n",
        "\n",
        "- Use a Gaussian process to act as a \"virtual sensor\"\n",
        "\n",
        "![](figs/aircraft%20wing/wing-vs.png)\n",
        "\n",
        "## Virtual sensing of different phenomena\n",
        "\n",
        "- We can extend virtual sensing to using one type of measurement to predict another\n",
        "\n",
        "- e.g predicting a wind turbine's power output from measured wind speed \n",
        "\n",
        "\n",
        "## Virtual sensing of different phenomena\n",
        "\n",
        "::: {layout-ncol=2 layout-valign=\"center\"}\n",
        "\n",
        "![](figs/wind-turbine-small.jpg){width=450}\n",
        "\n",
        "![](figs/powerCurve.png){width=450}\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Many other reasons\n",
        "\n",
        "- Surrogate modelling\n",
        "\n",
        "- Dealing with complex behaviours that we don't have physical models for\n",
        "\n",
        "- 'Digital twins'\n",
        "\n",
        "## Commonality across tasks\n",
        "\n",
        "- All revolve around learning a map from $X \\rightarrow \\mathbf{y}$\n",
        "\n",
        "- In machine learning & statistics, we call this a regression problem\n",
        "\n",
        "\n",
        "## Commonality across tasks\n",
        "\n",
        "- We can use Gaussian process (regression) to learn this map!\n",
        "\n",
        "- Boils down to fitting curves (functions) through data\n",
        "\n",
        "- **First Gaussian process definition**; a tool for fitting curves through data\n",
        "\n",
        "## Fitting curves\n"
      ],
      "id": "d734e33a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "from numpy.random import multivariate_normal\n",
        "import random\n",
        "\n",
        "import sys \n",
        "sys.path.append('../')\n",
        "\n",
        "import kernels\n",
        "import models\n",
        "\n",
        "np.random.seed(20)\n",
        "x = np.linspace(0,1,100)[:,None]\n",
        "\n",
        "hyps = np.array([1,.1,1e-6])\n",
        "kernel = kernels.SquaredExponential(x,x,hyps)\n",
        "k, _ , _ = kernel.compute_kernel()\n",
        "\n",
        "mu = np.zeros((100,))\n",
        "nsamples = 1\n",
        "y = multivariate_normal(mu, k,nsamples).T\n",
        "\n",
        "npts = 10\n",
        "random.seed(1)\n",
        "randidx = random.sample(range(0,100),npts)\n",
        "\n",
        "xsample = x[randidx]\n",
        "ysample = y[randidx]\n",
        "\n",
        "plt.plot(xsample,ysample,'kx')"
      ],
      "id": "6a39d061",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fitting a GP\n"
      ],
      "id": "7cacdcf7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "D = (xsample,ysample)\n",
        "mu_pred, cov_pred = models.predict(hyps,D,x,kernels.SquaredExponential)\n",
        "var_pred = np.diag(cov_pred)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(xsample,ysample,'kx',label ='train pts')\n",
        "plt.plot(x,mu_pred, label ='GP predictions')\n",
        "plt.fill_between(x[:,0], mu_pred[:,0] - 3*np.sqrt(var_pred), mu_pred[:,0] + 3*np.sqrt(var_pred),color = 'darkgreen', alpha = 0.3, label ='confidence bounds')\n",
        "plt.legend();"
      ],
      "id": "c0d75ff4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fitting a GP\n"
      ],
      "id": "60ba093f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "nsamples = 10\n",
        "gp_samples = multivariate_normal(mu, cov_pred, nsamples).T\n",
        "gp_samples += mu_pred\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(x,gp_samples)\n",
        "plt.plot(xsample,ysample,'kx',label ='train pts')\n",
        "plt.title('Posterior GP samples')\n",
        "plt.legend();\n"
      ],
      "id": "cd6606c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## So how do GPs do it?\n",
        "\n",
        "- You have a friend that asks you how your beer tastes..\n",
        "\n",
        "![](figs/hzy-ipa.webp){fig-align=\"center\" width=100}\n",
        "\n",
        "\n",
        "## So how do GPs do it?\n",
        "\n",
        "::: {layout-ncol=2}\n",
        "\n",
        "![](figs/hzy-ipa.webp){width=250}\n",
        "\n",
        "![](figs/beer-recipe.jpg){width=350}\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        "## So how do GPs do it?\n",
        "\n",
        "::: {layout-ncol=2}\n",
        "\n",
        "![](figs/hzy-ipa.webp){width=250}\n",
        "\n",
        "![](figs/hzy-ipa2.webp){width=450}\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        "## So how do GPs do it?\n",
        "\n",
        "::: {layout-ncol=2}\n",
        "\n",
        "![](figs/hzy-ipa.webp){width=250}\n",
        "\n",
        "![](figs/guiness.jpg){width=300}\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        "## A second definition\n",
        "\n",
        "Gaussian processes are a flexible tool for performing nonliner regression in a non-parametric, Bayesian manner\n",
        "\n",
        "## The intersection between beer tasting and GPs\n",
        "\n",
        "- GPs work in the same way! Use similarity between data points to tell us about others\n",
        "\n",
        "## The intersection between beer tasting and GPs\n",
        "\n",
        "- GPs work in the same way! Use similarity between data points to tell us about others\n",
        "\n",
        "- Basis of this notion is treating each data point as a Gaussian distribution\n",
        "\n",
        "- This mean we have some mean value, as well as some uncertainty on observed data \n",
        "\n",
        "\n",
        "## The Gaussian distribution \n"
      ],
      "id": "7cd8bb1a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#initialize a normal distribution with frozen in mean=-1, std. dev.= 1\n",
        "gaussian1 = norm(loc = 0., scale = 1.0)\n",
        "\n",
        "x = np.arange(-10, 10, .1)\n",
        "\n",
        "#plot the pdfs of these normal distributions \n",
        "plt.plot(x, gaussian1.pdf(x));\n",
        "plt.ylabel('p(x)');\n",
        "plt.xlabel('x');"
      ],
      "id": "b98025b0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$\n",
        "\n",
        "\n",
        "## The Gaussian distribution\n"
      ],
      "id": "3f05af9c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian2 = norm(loc = 0., scale = 1)\n",
        "gaussian3 = norm(loc = 2, scale = 2.5)\n",
        "\n",
        "plt.plot(x, gaussian2.pdf(x), x, gaussian3.pdf(x));\n",
        "plt.ylabel('p(x)');\n",
        "plt.xlabel('x');"
      ],
      "id": "d1d63e76",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$\n",
        "\n",
        "\n",
        "## Measuring similarity between the data\n"
      ],
      "id": "3d658b2d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mu = np.array([0,0])\n",
        "cov_ind = np.array([[1,0], [0,1]])\n",
        "cov_cor = np.array([[1,0.85],[0.85,1]])\n",
        "\n",
        "samples_ind = multivariate_normal(mu,cov_ind, size = 1000)\n",
        "samples_cor = multivariate_normal(mu,cov_cor, size = 1000)\n",
        "\n",
        "fig, axs = plt.subplots(1,2, figsize=(10,6))\n",
        "axs[0].plot(samples_ind[:,0], samples_ind[:,1],'x', color = 'indigo')\n",
        "axs[1].plot(samples_cor[:,0], samples_cor[:,1],'x', color = 'indigo')\n",
        "plt.setp(axs, xlim=(-4,4), ylim=(-4,4));\n",
        "\n",
        "fig.add_subplot(111, frameon=False)\n",
        "# hide tick and tick label of the big axes\n",
        "plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
        "plt.grid(False)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")"
      ],
      "id": "43e86801",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Beyond 2 data points\n",
        "\n",
        "d = 2\n"
      ],
      "id": "18dde0ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(20)\n",
        "x = np.linspace(0,1,100)[:,None]\n",
        "\n",
        "hyps = np.array([1,.25])\n",
        "kernel = kernels.SquaredExponential(x,x,hyps)\n",
        "k, _ , _ = kernel.compute_kernel()\n",
        "\n",
        "mu = np.zeros((100,))\n",
        "nsamples = 5\n",
        "y = multivariate_normal(mu, k,nsamples).T\n",
        "\n",
        "d = 2\n",
        "ysample = y[0::100//d,:]\n",
        "index = np.linspace(1,d,d)\n",
        "\n",
        "plt.plot(index,ysample)"
      ],
      "id": "a51e794d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Beyond 2 data points\n",
        "\n",
        "d = 4\n"
      ],
      "id": "a9fc56d2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d = 4\n",
        "ysample = y[0::100//d,:]\n",
        "index = np.linspace(1,d,d)\n",
        "\n",
        "plt.plot(index,ysample)"
      ],
      "id": "40b130e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Beyond 2 data points\n",
        "\n",
        "d = 5\n"
      ],
      "id": "516cd9ba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d = 5\n",
        "ysample = y[0::100//d,:]\n",
        "index = np.linspace(1,d,d)\n",
        "\n",
        "plt.plot(index,ysample)"
      ],
      "id": "e1ba1276",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Beyond 2 data points\n",
        "\n",
        "d = 10\n"
      ],
      "id": "e0ba8529"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d = 10\n",
        "ysample = y[0::100//d,:]\n",
        "index = np.linspace(1,d,d)\n",
        "\n",
        "plt.plot(index,ysample)"
      ],
      "id": "18f3dc7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Beyond 2 data points\n",
        "\n",
        "d = 20\n"
      ],
      "id": "19d2a8e0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d = 20\n",
        "ysample = y[0::100//d,:]\n",
        "index = np.linspace(1,d,d)\n",
        "\n",
        "plt.plot(index,ysample)"
      ],
      "id": "72e13856",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Beyond 2 data points\n",
        "\n",
        "d = 50\n"
      ],
      "id": "fb1b0098"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d = 50\n",
        "ysample = y[0::100//d,:]\n",
        "index = np.linspace(1,d,d)\n",
        "\n",
        "plt.plot(index,ysample)"
      ],
      "id": "1fd76f11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A third & final definition\n",
        "\n",
        "A Gaussian processes is a collection of random variables such that every subset of those random variables has a multivariate normal distribution\n"
      ],
      "id": "4f6b379b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d = 100\n",
        "ysample = y[0::100//d,:]\n",
        "index = np.linspace(1,d,d)\n",
        "\n",
        "plt.plot(index,ysample)"
      ],
      "id": "26d0494c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The core ingredients of a GP\n",
        "\n",
        "- To construct a Gaussian process model, we need to define both \n",
        "a mean and a covariance function \n",
        "\n",
        "- $f(\\mathbf{x}) \\sim \\mathcal{GP}( m(\\mathbf{x}), k (\\mathbf{x},\\mathbf{x}'))$\n",
        "\n",
        "## The core ingredients of a GP\n",
        "\n",
        "- Mean can be viewed as an offset of predictions\n",
        "\n",
        "- Covariance function controls how \"closeness\" of data points relates to their function value \n",
        "\n",
        "## The covariance function\n",
        "\n",
        "- Will generally assume a zero mean function\n",
        "\n",
        "- Many different types of kernel functions!\n",
        "\n",
        "- Loosely, they govern the \"shape\" of functions that the GP will model\n",
        "\n",
        "## Some examples \n"
      ],
      "id": "096469a3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hyps = [1,0.5]\n",
        "\n",
        "kernelSE = kernels.SquaredExponential(x,x,hyps)\n",
        "kernelMA12 = kernels.Matern12(x,x,hyps)\n",
        "\n",
        "kSE, _ , _ = kernelSE.compute_kernel()\n",
        "kMA12, _ , _ = kernelMA12.compute_kernel()\n",
        "\n",
        "mu = np.zeros((100,))\n",
        "nsamples = 10\n",
        "\n",
        "ySE = multivariate_normal(mu, kSE,nsamples).T\n",
        "yMA12 = multivariate_normal(mu, kMA12,nsamples).T\n",
        "\n",
        "fig, axs = plt.subplots(1,2, figsize=(10,6))\n",
        "axs[0].plot(x,ySE)\n",
        "axs[0].set_title('Samples from SE kernel');\n",
        "axs[1].plot(x,yMA12)\n",
        "axs[1].set_title('Samples from Matern 1/2 kernel');"
      ],
      "id": "4b469b44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Some examples \n"
      ],
      "id": "3fcb3268"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hyps = [1,0.5]\n",
        "\n",
        "kernelMA32 = kernels.Matern32(x,x,hyps)\n",
        "kernelMA12 = kernels.Matern12(x,x,hyps)\n",
        "\n",
        "kMA32, _ , _ = kernelMA32.compute_kernel()\n",
        "kMA12, _ , _ = kernelMA12.compute_kernel()\n",
        "\n",
        "mu = np.zeros((100,))\n",
        "nsamples = 10\n",
        "\n",
        "yMA12 = multivariate_normal(mu, kMA12,nsamples).T\n",
        "yMA32 = multivariate_normal(mu, kMA32,nsamples).T\n",
        "\n",
        "fig, axs = plt.subplots(1,2, figsize=(10,6))\n",
        "axs[0].plot(x,yMA12)\n",
        "axs[0].set_title('Samples from Matern 1/2 kernel');\n",
        "axs[1].plot(x,yMA32)\n",
        "axs[1].set_title('Samples from Matern 3/2 kernel');"
      ],
      "id": "0ed0c0b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## If we have more insight\n"
      ],
      "id": "65e1b756"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hyps = [1,0.5,0.5]\n",
        "\n",
        "kernelPeriodic = kernels.Periodic(x,x,hyps)\n",
        "\n",
        "kPeriodic, _ , _ = kernelPeriodic.compute_kernel()\n",
        "\n",
        "mu = np.zeros((100,))\n",
        "nsamples = 10\n",
        "\n",
        "yPeriodic = multivariate_normal(mu, kPeriodic,nsamples).T\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(x,yPeriodic)\n",
        "plt.title('Samples from periodic kernel ');"
      ],
      "id": "cb3f8134",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Breaking down the kernel\n",
        "\n",
        "- $d = || \\mathbf{x} - \\mathbf{x}' ||$, where $||\\mathbf{p},\\mathbf{q}|| = \\sqrt{\\sum_i{(q_i - p_i)}^2}$\n"
      ],
      "id": "4f3ead2d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define the two 2D points\n",
        "x1, y1 = 2, 3\n",
        "x2, y2 = 6, 7\n",
        "eqx, eqy = 4,3.5\n",
        "\n",
        "middle_x = (x1 + x2) / 2\n",
        "middle_y = (y1 + y2) / 2\n",
        "distance = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5\n",
        "\n",
        "# Plot the two points and the line connecting them\n",
        "label_offset = 0.25\n",
        "plt.plot([x1, x2], [y1, y2], 'go-',markersize = 8)\n",
        "plt.text(x1 + 0.5*label_offset, y1 + 1.5*label_offset, f'Point 1 \\n({x1},{y1})', ha='center')\n",
        "plt.text(x2 - 1.8*label_offset, y2 - 1*label_offset, f'Point 2 \\n({x2},{y2})', ha='center')\n",
        "\n",
        "plt.text(middle_x, middle_y + 1.5*label_offset, f'd', ha='center', va='top')\n",
        "plt.text(eqx, eqy, 'd = $\\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$', ha = 'left')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('x');\n",
        "plt.ylabel('y');"
      ],
      "id": "32fffbca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Breaking down the kernel\n",
        "\n",
        "- $d = || \\mathbf{x} - \\mathbf{x}' ||$, where $||\\mathbf{p},\\mathbf{q}|| = \\sqrt{\\sum_i{(q_i - p_i)}^2}$\n",
        "\n",
        "<br/>\n",
        "\n",
        "- $K_{SE}(\\mathbf{x}, \\mathbf{x}') = \\sigma _f^2 \\exp( - \\frac{d^2}{2 l^2})$\n",
        "\n",
        "<br/>\n",
        "\n",
        "- $K_{MA12}(\\mathbf{x}, \\mathbf{x}') = \\sigma _f^2 \\exp( -\\frac{d}{l})$\n",
        "\n",
        "<br/>\n",
        "\n",
        "- $K_{MA32}(\\mathbf{x}, \\mathbf{x}') = \\sigma _f^2 ( 1+ \\frac{\\sqrt{3}d}{l})\\exp( - \\frac{\\sqrt{3}d}{l})$\n",
        "\n",
        "## Hyperparameters \n",
        "\n",
        "- For a given kernel, there will be some hyperparameters that needed to be selected \n",
        "\n",
        "- For both the Squared Expoential and Matern family of kernels, these are the lengthscale ($l$) and signal variance $(\\sigma _f$)\n",
        "\n",
        "## Hyperparameters \n",
        "\n",
        "- \n",
        "\n",
        "## How do we set hyperparameters?\n",
        "\n",
        "- Generally won't know what these values should be...\n",
        "\n",
        "## Summary \n",
        "\n",
        "1. Acquire training data $D =(X,\\mathbf{y})$\n",
        "2. Select mean and kernel (covariance function)  \n",
        "    - if have prior knowledge, can put it in here!\n",
        "3. Learn hyperparameters by training the model via minimisation of the *negative marginal log likelihood*\n",
        "4. Make predictions by conditioning the model on the training data (shrinking possible functions where we have data)\n",
        "5. Take over the world! \n",
        "\n",
        "## Draw backs of GPs\n",
        "\n",
        ".. and there are several despite what you might think I believe after the last hour..\n",
        "\n",
        "- Training & memory requirements - scale very badly as we increase number of training points\n",
        "- Selecting a covariance function\n",
        "- Hyperparameter learning can be challenging; local optima, for more complex kernel structures & if we try and recover full distributions over the parameters \n",
        "\n",
        "## Bayes rule for GPs\n",
        "\n",
        "- Prior: $p(f) \\sim \\mathcal{N}(0,k(x,x'))$\n",
        "- Likelihood $p(y|f,X) \\sim \\mathcal{N}(f,\\sigma_n^2\\mathbb{I})$ - data\n",
        "- Marginal likelihood $p(y|X)$ - hyperparameter optimisation\n",
        "- Posterior = Bayes rule = $p(f|y,X) = \\frac{\\mathcal{N}(0,k(x,x'))\\mathcal{N}(f,\\sigma_n^2\\mathbb{I})}{p(y|X)}$\n"
      ],
      "id": "7d6df2bf"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}